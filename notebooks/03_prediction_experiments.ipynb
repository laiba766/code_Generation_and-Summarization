{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Prediction Experiments\n",
    "## Code Summarization and Generation Project\n",
    "\n",
    "This notebook implements and evaluates prediction models:\n",
    "- Random Forest\n",
    "- LSTM\n",
    "- Transformer (CodeBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from prediction_models import RandomForestPipeline, LSTMPipeline\n",
    "from visualization import ClusteringVisualizer, MetricsVisualizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "df = pd.read_csv('../data/processed/all_features.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Languages: {df['language'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Task Definition\n",
    "\n",
    "We'll predict the programming language based on code complexity features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and features\n",
    "target_col = 'language'\n",
    "\n",
    "# Select complexity features\n",
    "complexity_features = [\n",
    "    'ast_node_count', 'ast_depth', 'ast_leaf_count', 'ast_branching_factor_avg',\n",
    "    'ast_distinct_node_types', 'cc_mccabe', 'halstead_volume', 'halstead_difficulty',\n",
    "    'halstead_effort', 'num_if', 'num_for', 'num_while', 'loc'\n",
    "]\n",
    "\n",
    "print(f\"Predicting: {target_col}\")\n",
    "print(f\"Using {len(complexity_features)} complexity features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline = RandomForestPipeline(config, task='classification')\n",
    "\n",
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test = rf_pipeline.prepare_data(\n",
    "    df, target_col, feature_cols=complexity_features\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "train_results = rf_pipeline.train(X_train, y_train)\n",
    "\n",
    "print(\"\\nTraining Results:\")\n",
    "print(f\"Best parameters: {train_results['best_params']}\")\n",
    "print(f\"Best CV score: {train_results['best_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_results = rf_pipeline.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"\\nTest Set Evaluation:\")\n",
    "print(f\"Accuracy: {test_results['accuracy']:.4f}\")\n",
    "print(f\"Precision: {test_results['precision']:.4f}\")\n",
    "print(f\"Recall: {test_results['recall']:.4f}\")\n",
    "print(f\"F1-Score: {test_results['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "if 'classification_report' in test_results:\n",
    "    report_df = pd.DataFrame(test_results['classification_report']).transpose()\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "importance_df = rf_pipeline.get_feature_importance(complexity_features, top_k=15)\n",
    "\n",
    "print(\"\\nTop 15 Most Important Features:\")\n",
    "print(importance_df)\n",
    "\n",
    "# Plot\n",
    "visualizer = ClusteringVisualizer(config)\n",
    "visualizer.plot_feature_importance(\n",
    "    complexity_features,\n",
    "    np.array(test_results['feature_importance']),\n",
    "    top_k=15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_pipeline.model.predict(X_test)\n",
    "labels = rf_pipeline.label_encoder.classes_\n",
    "\n",
    "visualizer.plot_confusion_matrix(y_test, y_pred, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LSTM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pipeline = LSTMPipeline(config)\n",
    "\n",
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test, input_size, num_classes = lstm_pipeline.prepare_data(\n",
    "    df, target_col, feature_cols=complexity_features\n",
    ")\n",
    "\n",
    "print(f\"Input size: {input_size}\")\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM\n",
    "lstm_history = lstm_pipeline.train(X_train, y_train, input_size, num_classes)\n",
    "\n",
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax1.plot(lstm_history['train_loss'])\n",
    "ax1.set_title('LSTM Training Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(lstm_history['train_acc'])\n",
    "ax2.set_title('LSTM Training Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/visualizations/lstm_training_history.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LSTM\n",
    "lstm_results = lstm_pipeline.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"\\nLSTM Test Set Evaluation:\")\n",
    "print(f\"Accuracy: {lstm_results['accuracy']:.4f}\")\n",
    "print(f\"Precision: {lstm_results['precision']:.4f}\")\n",
    "print(f\"Recall: {lstm_results['recall']:.4f}\")\n",
    "print(f\"F1-Score: {lstm_results['f1_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'LSTM'],\n",
    "    'Accuracy': [test_results['accuracy'], lstm_results['accuracy']],\n",
    "    'Precision': [test_results['precision'], lstm_results['precision']],\n",
    "    'Recall': [test_results['recall'], lstm_results['recall']],\n",
    "    'F1-Score': [test_results['f1_score'], lstm_results['f1_score']]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv('../results/metrics/model_comparison.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "comparison_df_melted = comparison_df.melt(id_vars='Model', var_name='Metric', value_name='Score')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=comparison_df_melted, x='Metric', y='Score', hue='Model')\n",
    "plt.title('Model Performance Comparison', fontsize=14)\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('Score')\n",
    "plt.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/visualizations/model_comparison.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "all_results = {\n",
    "    'random_forest': {\n",
    "        'train': train_results,\n",
    "        'test': test_results\n",
    "    },\n",
    "    'lstm': {\n",
    "        'history': lstm_history,\n",
    "        'test': lstm_results\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('../results/metrics/prediction_results.json', 'w') as f:\n",
    "    json.dump(all_results, f, indent=2, default=lambda x: float(x) if isinstance(x, (np.floating, np.integer)) else x)\n",
    "\n",
    "print(\"Results saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
